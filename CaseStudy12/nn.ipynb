{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "First get the data.  Sklearn presents the data in a dictionary.  \n",
    "\n",
    "The target is the price is thousands\n",
    "\n",
    "Note that X is a dataframe while y is an array!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': '/Users/Yejur/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/boston_house_prices.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(bos['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns=bos['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bos['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data\n",
    "\n",
    "Neural Networks are especially sensitive do data scaling.  Nearly all the activation functions saturate at (0,1) or (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: median values were scaled by multiplying by 0.0275938190 and adding -0.047737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train = scaler.fit_transform(x)\n",
    "\n",
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "print(\"Note: median values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[12], scaler.min_[12]))\n",
    "multiplied_by = scaler.scale_[12]\n",
    "added = scaler.min_[12]\n",
    "\n",
    "scaled_train_df = pd.DataFrame(scaled_train, columns=x.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in scaled_train_df:\n",
    "#     scaled_train_df[i].hist()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Alright lets get down to business.  We will use the Sequential() class to build a model.  This means we just add layer after layer to the model (in order).\n",
    "\n",
    "Note that this is NOT the keras package proper.  This is an implementation of the Keras API in tensorflow.  THey are closely related, but this IS Tensorflow (package), not Keras (package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 21:11:12.574403 4386702784 deprecation.py:506] From /Users/Yejur/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, activation='sigmoid'))\n",
    "model.add(layers.Dense(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "The compile step adds an optimizer (solver) and your loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model\n",
    "Now it is time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "506/506 [==============================] - 0s 203us/sample - loss: 239.6423 - mean_squared_error: 239.6423\n",
      "Epoch 2/10\n",
      "506/506 [==============================] - 0s 100us/sample - loss: 85.2276 - mean_squared_error: 85.2276\n",
      "Epoch 3/10\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 78.2300 - mean_squared_error: 78.2300\n",
      "Epoch 4/10\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 68.7985 - mean_squared_error: 68.7985\n",
      "Epoch 5/10\n",
      "506/506 [==============================] - 0s 56us/sample - loss: 60.9361 - mean_squared_error: 60.9361\n",
      "Epoch 6/10\n",
      "506/506 [==============================] - 0s 120us/sample - loss: 56.0301 - mean_squared_error: 56.0301\n",
      "Epoch 7/10\n",
      "506/506 [==============================] - 0s 140us/sample - loss: 52.9354 - mean_squared_error: 52.9354\n",
      "Epoch 8/10\n",
      "506/506 [==============================] - 0s 105us/sample - loss: 50.1574 - mean_squared_error: 50.1574\n",
      "Epoch 9/10\n",
      "506/506 [==============================] - 0s 217us/sample - loss: 47.9368 - mean_squared_error: 47.9368\n",
      "Epoch 10/10\n",
      "506/506 [==============================] - 0s 248us/sample - loss: 45.5054 - mean_squared_error: 45.5054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2b89f320>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_train_df.values, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize that data\n",
    "Tensorboard allows you to look at your training curves in real time.  All we need to do here is add a \"callback\" that is executed every epoch end.  Tensorboard writes a summary of the model output.  If you manually build a neural network, you need to add tf.summary operations.\n",
    "\n",
    "To run tensorboard type\n",
    "\n",
    "`tensorboard --logdir logs`\n",
    "\n",
    "Then go to localhost:6006 in your web browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tb = TensorBoard(log_dir=f\"logs\\\\{time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 0s 209us/sample - loss: 43.9508 - mean_squared_error: 43.9508\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s 107us/sample - loss: 43.5864 - mean_squared_error: 43.5864\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s 157us/sample - loss: 43.2202 - mean_squared_error: 43.2202\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s 251us/sample - loss: 42.8723 - mean_squared_error: 42.8723\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s 313us/sample - loss: 42.5450 - mean_squared_error: 42.5450\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s 228us/sample - loss: 42.2130 - mean_squared_error: 42.2130\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s 193us/sample - loss: 41.8814 - mean_squared_error: 41.8814\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 176us/sample - loss: 41.5850 - mean_squared_error: 41.5850\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s 867us/sample - loss: 41.2868 - mean_squared_error: 41.2868\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s 447us/sample - loss: 40.9418 - mean_squared_error: 40.9418\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 647us/sample - loss: 40.6263 - mean_squared_error: 40.6263\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 157us/sample - loss: 40.3207 - mean_squared_error: 40.3207\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s 176us/sample - loss: 39.9817 - mean_squared_error: 39.9817\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 148us/sample - loss: 39.6853 - mean_squared_error: 39.6853\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s 182us/sample - loss: 39.3739 - mean_squared_error: 39.3739\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 176us/sample - loss: 39.0660 - mean_squared_error: 39.0660\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s 224us/sample - loss: 38.7709 - mean_squared_error: 38.7709\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 250us/sample - loss: 38.4710 - mean_squared_error: 38.4710\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 158us/sample - loss: 38.1548 - mean_squared_error: 38.1548\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 178us/sample - loss: 37.8364 - mean_squared_error: 37.8364\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s 199us/sample - loss: 37.5814 - mean_squared_error: 37.5814\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 174us/sample - loss: 37.2911 - mean_squared_error: 37.2911\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 126us/sample - loss: 36.9640 - mean_squared_error: 36.9640\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 137us/sample - loss: 36.6647 - mean_squared_error: 36.6647\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 124us/sample - loss: 36.4113 - mean_squared_error: 36.4113\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 179us/sample - loss: 36.1290 - mean_squared_error: 36.1290\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s 597us/sample - loss: 35.8593 - mean_squared_error: 35.8593\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 402us/sample - loss: 35.5989 - mean_squared_error: 35.5989\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 491us/sample - loss: 35.3681 - mean_squared_error: 35.3681\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 218us/sample - loss: 35.0251 - mean_squared_error: 35.0251\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 156us/sample - loss: 34.7666 - mean_squared_error: 34.7666\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 137us/sample - loss: 34.5723 - mean_squared_error: 34.5723\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 137us/sample - loss: 34.2616 - mean_squared_error: 34.2616\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s 144us/sample - loss: 33.9677 - mean_squared_error: 33.9677\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 164us/sample - loss: 33.7136 - mean_squared_error: 33.7136\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 202us/sample - loss: 33.4725 - mean_squared_error: 33.4725\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 248us/sample - loss: 33.2202 - mean_squared_error: 33.2202\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 173us/sample - loss: 33.0077 - mean_squared_error: 33.0077\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 134us/sample - loss: 32.7256 - mean_squared_error: 32.7256\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 172us/sample - loss: 32.4871 - mean_squared_error: 32.4871\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s 661us/sample - loss: 32.2689 - mean_squared_error: 32.2689\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s 175us/sample - loss: 32.0342 - mean_squared_error: 32.0342\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s 656us/sample - loss: 31.7922 - mean_squared_error: 31.7922\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 313us/sample - loss: 31.5416 - mean_squared_error: 31.5416\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 0s 196us/sample - loss: 31.3587 - mean_squared_error: 31.3587\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 0s 163us/sample - loss: 31.0849 - mean_squared_error: 31.0849\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s 174us/sample - loss: 30.8619 - mean_squared_error: 30.8619\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 150us/sample - loss: 30.6392 - mean_squared_error: 30.6392\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 145us/sample - loss: 30.4060 - mean_squared_error: 30.4060\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s 425us/sample - loss: 30.2182 - mean_squared_error: 30.2182\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 382us/sample - loss: 29.9758 - mean_squared_error: 29.9758\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 245us/sample - loss: 29.7915 - mean_squared_error: 29.7915\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 528us/sample - loss: 29.6197 - mean_squared_error: 29.6197\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s 283us/sample - loss: 29.4145 - mean_squared_error: 29.4145\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 0s 215us/sample - loss: 29.2538 - mean_squared_error: 29.2538\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s 216us/sample - loss: 28.9497 - mean_squared_error: 28.9497\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s 158us/sample - loss: 28.8550 - mean_squared_error: 28.8550\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s 187us/sample - loss: 28.6523 - mean_squared_error: 28.6523\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s 302us/sample - loss: 28.4328 - mean_squared_error: 28.4328\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 205us/sample - loss: 28.2797 - mean_squared_error: 28.2797\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 147us/sample - loss: 28.0717 - mean_squared_error: 28.0717\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 175us/sample - loss: 27.8907 - mean_squared_error: 27.8907\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 219us/sample - loss: 27.7000 - mean_squared_error: 27.7001\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 140us/sample - loss: 27.5029 - mean_squared_error: 27.5029\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 104us/sample - loss: 27.4225 - mean_squared_error: 27.4225\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s 185us/sample - loss: 27.2218 - mean_squared_error: 27.2218\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 636us/sample - loss: 27.0128 - mean_squared_error: 27.0128\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 217us/sample - loss: 26.8668 - mean_squared_error: 26.8668\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s 306us/sample - loss: 26.6947 - mean_squared_error: 26.6947\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s 525us/sample - loss: 26.4785 - mean_squared_error: 26.4785\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 224us/sample - loss: 26.3856 - mean_squared_error: 26.3856\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 276us/sample - loss: 26.2225 - mean_squared_error: 26.2225\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 0s 117us/sample - loss: 26.0747 - mean_squared_error: 26.0747\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 158us/sample - loss: 25.9227 - mean_squared_error: 25.9227\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 212us/sample - loss: 25.7820 - mean_squared_error: 25.7820\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 181us/sample - loss: 25.6812 - mean_squared_error: 25.6812\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 316us/sample - loss: 25.5153 - mean_squared_error: 25.5153\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s 313us/sample - loss: 25.3613 - mean_squared_error: 25.3613\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s 173us/sample - loss: 25.2623 - mean_squared_error: 25.2623\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 0s 182us/sample - loss: 25.1318 - mean_squared_error: 25.1318\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s 195us/sample - loss: 24.9724 - mean_squared_error: 24.9724\n",
      "Epoch 82/100\n",
      "506/506 [==============================] - 0s 162us/sample - loss: 24.8445 - mean_squared_error: 24.8445\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 256us/sample - loss: 24.7576 - mean_squared_error: 24.7576\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 273us/sample - loss: 24.6063 - mean_squared_error: 24.6063\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 0s 177us/sample - loss: 24.4675 - mean_squared_error: 24.4676\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s 198us/sample - loss: 24.3716 - mean_squared_error: 24.3716\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s 161us/sample - loss: 24.2394 - mean_squared_error: 24.2394\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 195us/sample - loss: 24.1601 - mean_squared_error: 24.1601\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s 190us/sample - loss: 24.0146 - mean_squared_error: 24.0146\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 153us/sample - loss: 23.8775 - mean_squared_error: 23.8775\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s 199us/sample - loss: 23.8171 - mean_squared_error: 23.8171\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s 341us/sample - loss: 23.6754 - mean_squared_error: 23.6753\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s 674us/sample - loss: 23.5801 - mean_squared_error: 23.5801\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 497us/sample - loss: 23.4363 - mean_squared_error: 23.4363\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 0s 391us/sample - loss: 23.3346 - mean_squared_error: 23.3346\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s 156us/sample - loss: 23.2151 - mean_squared_error: 23.2151\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s 174us/sample - loss: 23.1364 - mean_squared_error: 23.1364\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 493us/sample - loss: 23.0475 - mean_squared_error: 23.0475\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 388us/sample - loss: 22.9493 - mean_squared_error: 22.9493\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s 432us/sample - loss: 22.8640 - mean_squared_error: 22.8640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2c46a2e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_train_df.values, y, epochs=100, batch_size=20, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0 at http://Erinas-MBP:6006/ (Press CTRL+C to quit)\n",
      "E0716 21:15:01.787928 123145391058944 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.341408 123145370038272 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.352252 123145370038272 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.435185 123145370038272 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.438903 123145375293440 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.459167 123145370038272 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.460433 123145375293440 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "E0716 21:15:02.501936 123145385803776 hparams_plugin.py:108] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate!!\n",
    "SO far we have used the entire dataset to train.  We know that Prof Slater frown upon this.  In Neural Networks, rahter than a cross validation, a validation split is done.  Lets repeat the experiment with splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_train_df, y, test_size=0.20, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = tf.keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "\n",
    "model_final.add(layers.Dense(100, activation='sigmoid'))\n",
    "model_final.add(layers.Dense(100, activation='sigmoid'))\n",
    "model_final.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples, validate on 102 samples\n",
      "Epoch 1/100\n",
      "404/404 [==============================] - 0s 598us/sample - loss: 257.9249 - mean_squared_error: 257.9250 - val_loss: 117.3650 - val_mean_squared_error: 117.3650\n",
      "Epoch 2/100\n",
      "404/404 [==============================] - 0s 216us/sample - loss: 88.1265 - mean_squared_error: 88.1265 - val_loss: 91.0296 - val_mean_squared_error: 91.0296\n",
      "Epoch 3/100\n",
      "404/404 [==============================] - 0s 957us/sample - loss: 83.7833 - mean_squared_error: 83.7833 - val_loss: 89.1834 - val_mean_squared_error: 89.1834\n",
      "Epoch 4/100\n",
      "404/404 [==============================] - 0s 356us/sample - loss: 83.4728 - mean_squared_error: 83.4728 - val_loss: 91.1977 - val_mean_squared_error: 91.1977\n",
      "Epoch 5/100\n",
      "404/404 [==============================] - 0s 326us/sample - loss: 83.3035 - mean_squared_error: 83.3035 - val_loss: 90.6190 - val_mean_squared_error: 90.6190\n",
      "Epoch 6/100\n",
      "404/404 [==============================] - 0s 220us/sample - loss: 83.1395 - mean_squared_error: 83.1395 - val_loss: 89.5973 - val_mean_squared_error: 89.5973\n",
      "Epoch 7/100\n",
      "404/404 [==============================] - 0s 211us/sample - loss: 83.0295 - mean_squared_error: 83.0295 - val_loss: 89.4548 - val_mean_squared_error: 89.4548\n",
      "Epoch 8/100\n",
      "404/404 [==============================] - 0s 804us/sample - loss: 82.7378 - mean_squared_error: 82.7378 - val_loss: 88.2566 - val_mean_squared_error: 88.2566\n",
      "Epoch 9/100\n",
      "404/404 [==============================] - 0s 206us/sample - loss: 82.7390 - mean_squared_error: 82.7390 - val_loss: 87.9914 - val_mean_squared_error: 87.9914\n",
      "Epoch 10/100\n",
      "404/404 [==============================] - 0s 184us/sample - loss: 82.6127 - mean_squared_error: 82.6127 - val_loss: 88.6165 - val_mean_squared_error: 88.6165\n",
      "Epoch 11/100\n",
      "404/404 [==============================] - 0s 271us/sample - loss: 82.3945 - mean_squared_error: 82.3945 - val_loss: 90.0379 - val_mean_squared_error: 90.0379\n",
      "Epoch 12/100\n",
      "404/404 [==============================] - 0s 850us/sample - loss: 82.3166 - mean_squared_error: 82.3166 - val_loss: 88.5349 - val_mean_squared_error: 88.5349\n",
      "Epoch 13/100\n",
      "404/404 [==============================] - 0s 563us/sample - loss: 82.2453 - mean_squared_error: 82.2453 - val_loss: 87.3576 - val_mean_squared_error: 87.3576\n",
      "Epoch 14/100\n",
      "404/404 [==============================] - 0s 231us/sample - loss: 81.7864 - mean_squared_error: 81.7864 - val_loss: 85.7052 - val_mean_squared_error: 85.7052\n",
      "Epoch 15/100\n",
      "404/404 [==============================] - 0s 386us/sample - loss: 81.6792 - mean_squared_error: 81.6792 - val_loss: 90.6896 - val_mean_squared_error: 90.6896\n",
      "Epoch 16/100\n",
      "404/404 [==============================] - 0s 286us/sample - loss: 82.0100 - mean_squared_error: 82.0100 - val_loss: 87.2038 - val_mean_squared_error: 87.2038\n",
      "Epoch 17/100\n",
      "404/404 [==============================] - 0s 781us/sample - loss: 81.5415 - mean_squared_error: 81.5415 - val_loss: 87.1322 - val_mean_squared_error: 87.1322\n",
      "Epoch 18/100\n",
      "404/404 [==============================] - 0s 440us/sample - loss: 81.5101 - mean_squared_error: 81.5101 - val_loss: 87.0387 - val_mean_squared_error: 87.0387\n",
      "Epoch 19/100\n",
      "404/404 [==============================] - 0s 241us/sample - loss: 81.3500 - mean_squared_error: 81.3500 - val_loss: 87.9695 - val_mean_squared_error: 87.9695\n",
      "Epoch 20/100\n",
      "404/404 [==============================] - 0s 234us/sample - loss: 80.5494 - mean_squared_error: 80.5494 - val_loss: 88.0982 - val_mean_squared_error: 88.0982\n",
      "Epoch 21/100\n",
      "404/404 [==============================] - 0s 977us/sample - loss: 80.8208 - mean_squared_error: 80.8208 - val_loss: 86.3110 - val_mean_squared_error: 86.3110\n",
      "Epoch 22/100\n",
      "404/404 [==============================] - 0s 515us/sample - loss: 80.4828 - mean_squared_error: 80.4827 - val_loss: 86.2302 - val_mean_squared_error: 86.2302\n",
      "Epoch 23/100\n",
      "404/404 [==============================] - 0s 710us/sample - loss: 80.2207 - mean_squared_error: 80.2207 - val_loss: 88.8715 - val_mean_squared_error: 88.8715\n",
      "Epoch 24/100\n",
      "404/404 [==============================] - 0s 394us/sample - loss: 79.7822 - mean_squared_error: 79.7822 - val_loss: 84.0884 - val_mean_squared_error: 84.0884\n",
      "Epoch 25/100\n",
      "404/404 [==============================] - 0s 222us/sample - loss: 79.8436 - mean_squared_error: 79.8436 - val_loss: 85.7097 - val_mean_squared_error: 85.7097\n",
      "Epoch 26/100\n",
      "404/404 [==============================] - 0s 254us/sample - loss: 79.5540 - mean_squared_error: 79.5540 - val_loss: 83.9395 - val_mean_squared_error: 83.9395\n",
      "Epoch 27/100\n",
      "404/404 [==============================] - 0s 293us/sample - loss: 79.2235 - mean_squared_error: 79.2235 - val_loss: 86.4797 - val_mean_squared_error: 86.4797\n",
      "Epoch 28/100\n",
      "404/404 [==============================] - 0s 294us/sample - loss: 78.5866 - mean_squared_error: 78.5866 - val_loss: 84.4803 - val_mean_squared_error: 84.4803\n",
      "Epoch 29/100\n",
      "404/404 [==============================] - 0s 464us/sample - loss: 78.7293 - mean_squared_error: 78.7293 - val_loss: 83.9334 - val_mean_squared_error: 83.9334\n",
      "Epoch 30/100\n",
      "404/404 [==============================] - 0s 240us/sample - loss: 78.0138 - mean_squared_error: 78.0138 - val_loss: 82.4263 - val_mean_squared_error: 82.4263\n",
      "Epoch 31/100\n",
      "404/404 [==============================] - 0s 755us/sample - loss: 78.3279 - mean_squared_error: 78.3279 - val_loss: 82.3871 - val_mean_squared_error: 82.3871\n",
      "Epoch 32/100\n",
      "404/404 [==============================] - 0s 302us/sample - loss: 77.0197 - mean_squared_error: 77.0197 - val_loss: 85.8012 - val_mean_squared_error: 85.8012\n",
      "Epoch 33/100\n",
      "404/404 [==============================] - 0s 293us/sample - loss: 76.8295 - mean_squared_error: 76.8295 - val_loss: 82.7923 - val_mean_squared_error: 82.7923\n",
      "Epoch 34/100\n",
      "404/404 [==============================] - 0s 306us/sample - loss: 76.6559 - mean_squared_error: 76.6559 - val_loss: 85.1231 - val_mean_squared_error: 85.1232\n",
      "Epoch 35/100\n",
      "404/404 [==============================] - 0s 341us/sample - loss: 76.1839 - mean_squared_error: 76.1839 - val_loss: 82.2240 - val_mean_squared_error: 82.2240\n",
      "Epoch 36/100\n",
      "404/404 [==============================] - 0s 346us/sample - loss: 75.7023 - mean_squared_error: 75.7023 - val_loss: 80.6492 - val_mean_squared_error: 80.6492\n",
      "Epoch 37/100\n",
      "404/404 [==============================] - 0s 605us/sample - loss: 75.4487 - mean_squared_error: 75.4487 - val_loss: 82.4902 - val_mean_squared_error: 82.4902\n",
      "Epoch 38/100\n",
      "404/404 [==============================] - 0s 247us/sample - loss: 74.5218 - mean_squared_error: 74.5218 - val_loss: 83.1489 - val_mean_squared_error: 83.1489\n",
      "Epoch 39/100\n",
      "404/404 [==============================] - 0s 236us/sample - loss: 73.9672 - mean_squared_error: 73.9672 - val_loss: 80.0263 - val_mean_squared_error: 80.0263\n",
      "Epoch 40/100\n",
      "404/404 [==============================] - 0s 273us/sample - loss: 73.6025 - mean_squared_error: 73.6025 - val_loss: 82.2592 - val_mean_squared_error: 82.2592\n",
      "Epoch 41/100\n",
      "404/404 [==============================] - 0s 254us/sample - loss: 72.8613 - mean_squared_error: 72.8613 - val_loss: 78.7985 - val_mean_squared_error: 78.7985\n",
      "Epoch 42/100\n",
      "404/404 [==============================] - 0s 233us/sample - loss: 72.4355 - mean_squared_error: 72.4355 - val_loss: 79.6323 - val_mean_squared_error: 79.6323\n",
      "Epoch 43/100\n",
      "404/404 [==============================] - 0s 277us/sample - loss: 71.6471 - mean_squared_error: 71.6471 - val_loss: 78.3542 - val_mean_squared_error: 78.3542\n",
      "Epoch 44/100\n",
      "404/404 [==============================] - 0s 387us/sample - loss: 71.2880 - mean_squared_error: 71.2880 - val_loss: 77.4562 - val_mean_squared_error: 77.4562\n",
      "Epoch 45/100\n",
      "404/404 [==============================] - 0s 225us/sample - loss: 70.2231 - mean_squared_error: 70.2231 - val_loss: 78.8378 - val_mean_squared_error: 78.8378\n",
      "Epoch 46/100\n",
      "404/404 [==============================] - 0s 269us/sample - loss: 69.3659 - mean_squared_error: 69.3659 - val_loss: 77.3934 - val_mean_squared_error: 77.3934\n",
      "Epoch 47/100\n",
      "404/404 [==============================] - 0s 323us/sample - loss: 68.6033 - mean_squared_error: 68.6033 - val_loss: 77.5867 - val_mean_squared_error: 77.5867\n",
      "Epoch 48/100\n",
      "404/404 [==============================] - 0s 298us/sample - loss: 67.7441 - mean_squared_error: 67.7441 - val_loss: 77.3371 - val_mean_squared_error: 77.3371\n",
      "Epoch 49/100\n",
      "404/404 [==============================] - 0s 283us/sample - loss: 66.9231 - mean_squared_error: 66.9231 - val_loss: 74.9839 - val_mean_squared_error: 74.9839\n",
      "Epoch 50/100\n",
      "404/404 [==============================] - 0s 233us/sample - loss: 66.2125 - mean_squared_error: 66.2125 - val_loss: 75.4918 - val_mean_squared_error: 75.4918\n",
      "Epoch 51/100\n",
      "404/404 [==============================] - 0s 211us/sample - loss: 64.7350 - mean_squared_error: 64.7350 - val_loss: 74.3735 - val_mean_squared_error: 74.3735\n",
      "Epoch 52/100\n",
      "404/404 [==============================] - 0s 219us/sample - loss: 65.5438 - mean_squared_error: 65.5438 - val_loss: 72.1034 - val_mean_squared_error: 72.1034\n",
      "Epoch 53/100\n",
      "404/404 [==============================] - 0s 239us/sample - loss: 63.5693 - mean_squared_error: 63.5693 - val_loss: 72.2678 - val_mean_squared_error: 72.2678\n",
      "Epoch 54/100\n",
      "404/404 [==============================] - 0s 425us/sample - loss: 62.3608 - mean_squared_error: 62.3608 - val_loss: 72.6876 - val_mean_squared_error: 72.6876\n",
      "Epoch 55/100\n",
      "404/404 [==============================] - 0s 601us/sample - loss: 61.1688 - mean_squared_error: 61.1688 - val_loss: 70.7950 - val_mean_squared_error: 70.7950\n",
      "Epoch 56/100\n",
      "404/404 [==============================] - 0s 572us/sample - loss: 60.3675 - mean_squared_error: 60.3675 - val_loss: 70.3025 - val_mean_squared_error: 70.3025\n",
      "Epoch 57/100\n",
      "404/404 [==============================] - 0s 1ms/sample - loss: 59.4369 - mean_squared_error: 59.4369 - val_loss: 71.4653 - val_mean_squared_error: 71.4653\n",
      "Epoch 58/100\n",
      "404/404 [==============================] - 0s 535us/sample - loss: 58.3771 - mean_squared_error: 58.3771 - val_loss: 69.7202 - val_mean_squared_error: 69.7202\n",
      "Epoch 59/100\n",
      "404/404 [==============================] - 0s 318us/sample - loss: 57.7889 - mean_squared_error: 57.7889 - val_loss: 70.7683 - val_mean_squared_error: 70.7683\n",
      "Epoch 60/100\n",
      "404/404 [==============================] - 0s 235us/sample - loss: 57.1094 - mean_squared_error: 57.1094 - val_loss: 69.0158 - val_mean_squared_error: 69.0158\n",
      "Epoch 61/100\n",
      "404/404 [==============================] - 0s 247us/sample - loss: 55.7369 - mean_squared_error: 55.7369 - val_loss: 69.1648 - val_mean_squared_error: 69.1648\n",
      "Epoch 62/100\n",
      "404/404 [==============================] - 0s 296us/sample - loss: 54.9322 - mean_squared_error: 54.9322 - val_loss: 67.1145 - val_mean_squared_error: 67.1145\n",
      "Epoch 63/100\n",
      "404/404 [==============================] - 0s 452us/sample - loss: 54.0297 - mean_squared_error: 54.0298 - val_loss: 65.9974 - val_mean_squared_error: 65.9974\n",
      "Epoch 64/100\n",
      "404/404 [==============================] - 0s 430us/sample - loss: 53.3853 - mean_squared_error: 53.3853 - val_loss: 65.4567 - val_mean_squared_error: 65.4567\n",
      "Epoch 65/100\n",
      "404/404 [==============================] - 0s 233us/sample - loss: 52.7172 - mean_squared_error: 52.7172 - val_loss: 68.0373 - val_mean_squared_error: 68.0373\n",
      "Epoch 66/100\n",
      "404/404 [==============================] - 0s 223us/sample - loss: 52.1420 - mean_squared_error: 52.1420 - val_loss: 64.0677 - val_mean_squared_error: 64.0677\n",
      "Epoch 67/100\n",
      "404/404 [==============================] - 0s 278us/sample - loss: 51.0030 - mean_squared_error: 51.0030 - val_loss: 63.3491 - val_mean_squared_error: 63.3491\n",
      "Epoch 68/100\n",
      "404/404 [==============================] - 0s 245us/sample - loss: 50.6957 - mean_squared_error: 50.6957 - val_loss: 63.0789 - val_mean_squared_error: 63.0789\n",
      "Epoch 69/100\n",
      "404/404 [==============================] - 0s 227us/sample - loss: 49.9254 - mean_squared_error: 49.9254 - val_loss: 63.0035 - val_mean_squared_error: 63.0035\n",
      "Epoch 70/100\n",
      "404/404 [==============================] - 0s 269us/sample - loss: 48.9256 - mean_squared_error: 48.9256 - val_loss: 63.2621 - val_mean_squared_error: 63.2621\n",
      "Epoch 71/100\n",
      "404/404 [==============================] - 0s 238us/sample - loss: 48.5315 - mean_squared_error: 48.5315 - val_loss: 61.6364 - val_mean_squared_error: 61.6364\n",
      "Epoch 72/100\n",
      "404/404 [==============================] - 0s 209us/sample - loss: 48.0621 - mean_squared_error: 48.0621 - val_loss: 62.1845 - val_mean_squared_error: 62.1846\n",
      "Epoch 73/100\n",
      "404/404 [==============================] - 0s 377us/sample - loss: 47.5254 - mean_squared_error: 47.5254 - val_loss: 61.4740 - val_mean_squared_error: 61.4740\n",
      "Epoch 74/100\n",
      "404/404 [==============================] - 0s 239us/sample - loss: 47.1869 - mean_squared_error: 47.1869 - val_loss: 60.5056 - val_mean_squared_error: 60.5056\n",
      "Epoch 75/100\n",
      "404/404 [==============================] - 0s 585us/sample - loss: 46.4061 - mean_squared_error: 46.4061 - val_loss: 61.9971 - val_mean_squared_error: 61.9971\n",
      "Epoch 76/100\n",
      "404/404 [==============================] - 0s 623us/sample - loss: 46.1571 - mean_squared_error: 46.1571 - val_loss: 60.1020 - val_mean_squared_error: 60.1020\n",
      "Epoch 77/100\n",
      "404/404 [==============================] - 0s 703us/sample - loss: 45.5685 - mean_squared_error: 45.5685 - val_loss: 59.5304 - val_mean_squared_error: 59.5304\n",
      "Epoch 78/100\n",
      "404/404 [==============================] - 0s 1ms/sample - loss: 45.0627 - mean_squared_error: 45.0628 - val_loss: 60.3041 - val_mean_squared_error: 60.3041\n",
      "Epoch 79/100\n",
      "404/404 [==============================] - 0s 576us/sample - loss: 44.6915 - mean_squared_error: 44.6915 - val_loss: 59.1346 - val_mean_squared_error: 59.1346\n",
      "Epoch 80/100\n",
      "404/404 [==============================] - 0s 270us/sample - loss: 44.3765 - mean_squared_error: 44.3766 - val_loss: 59.2852 - val_mean_squared_error: 59.2852\n",
      "Epoch 81/100\n",
      "404/404 [==============================] - 0s 539us/sample - loss: 43.9223 - mean_squared_error: 43.9223 - val_loss: 58.6444 - val_mean_squared_error: 58.6444\n",
      "Epoch 82/100\n",
      "404/404 [==============================] - 0s 265us/sample - loss: 43.6238 - mean_squared_error: 43.6238 - val_loss: 59.1030 - val_mean_squared_error: 59.1030\n",
      "Epoch 83/100\n",
      "404/404 [==============================] - 0s 314us/sample - loss: 43.5121 - mean_squared_error: 43.5121 - val_loss: 59.4093 - val_mean_squared_error: 59.4093\n",
      "Epoch 84/100\n",
      "404/404 [==============================] - 0s 611us/sample - loss: 42.9219 - mean_squared_error: 42.9219 - val_loss: 58.0067 - val_mean_squared_error: 58.0067\n",
      "Epoch 85/100\n",
      "404/404 [==============================] - 0s 519us/sample - loss: 42.4914 - mean_squared_error: 42.4914 - val_loss: 57.4586 - val_mean_squared_error: 57.4586\n",
      "Epoch 86/100\n",
      "404/404 [==============================] - 0s 749us/sample - loss: 42.1836 - mean_squared_error: 42.1836 - val_loss: 56.9654 - val_mean_squared_error: 56.9654\n",
      "Epoch 87/100\n",
      "404/404 [==============================] - 0s 379us/sample - loss: 41.7809 - mean_squared_error: 41.7809 - val_loss: 56.2922 - val_mean_squared_error: 56.2922\n",
      "Epoch 88/100\n",
      "404/404 [==============================] - 0s 384us/sample - loss: 41.1365 - mean_squared_error: 41.1365 - val_loss: 56.0356 - val_mean_squared_error: 56.0356\n",
      "Epoch 89/100\n",
      "404/404 [==============================] - 0s 282us/sample - loss: 40.8202 - mean_squared_error: 40.8202 - val_loss: 55.7069 - val_mean_squared_error: 55.7069\n",
      "Epoch 90/100\n",
      "404/404 [==============================] - 0s 355us/sample - loss: 40.5783 - mean_squared_error: 40.5783 - val_loss: 55.3698 - val_mean_squared_error: 55.3698\n",
      "Epoch 91/100\n",
      "404/404 [==============================] - 0s 260us/sample - loss: 40.1505 - mean_squared_error: 40.1505 - val_loss: 54.7308 - val_mean_squared_error: 54.7308\n",
      "Epoch 92/100\n",
      "404/404 [==============================] - 0s 546us/sample - loss: 39.7083 - mean_squared_error: 39.7083 - val_loss: 56.1929 - val_mean_squared_error: 56.1929\n",
      "Epoch 93/100\n",
      "404/404 [==============================] - 0s 238us/sample - loss: 39.3297 - mean_squared_error: 39.3297 - val_loss: 55.0077 - val_mean_squared_error: 55.0077\n",
      "Epoch 94/100\n",
      "404/404 [==============================] - 0s 262us/sample - loss: 38.9860 - mean_squared_error: 38.9860 - val_loss: 54.9430 - val_mean_squared_error: 54.9430\n",
      "Epoch 95/100\n",
      "404/404 [==============================] - 0s 335us/sample - loss: 38.5925 - mean_squared_error: 38.5925 - val_loss: 53.3579 - val_mean_squared_error: 53.3579\n",
      "Epoch 96/100\n",
      "404/404 [==============================] - 0s 675us/sample - loss: 38.3787 - mean_squared_error: 38.3787 - val_loss: 52.9771 - val_mean_squared_error: 52.9771\n",
      "Epoch 97/100\n",
      "404/404 [==============================] - 0s 375us/sample - loss: 37.8863 - mean_squared_error: 37.8863 - val_loss: 52.5211 - val_mean_squared_error: 52.5211\n",
      "Epoch 98/100\n",
      "404/404 [==============================] - 0s 1ms/sample - loss: 37.5363 - mean_squared_error: 37.5363 - val_loss: 52.4426 - val_mean_squared_error: 52.4426\n",
      "Epoch 99/100\n",
      "404/404 [==============================] - 0s 174us/sample - loss: 37.2072 - mean_squared_error: 37.2072 - val_loss: 51.7615 - val_mean_squared_error: 51.7615\n",
      "Epoch 100/100\n",
      "404/404 [==============================] - 0s 167us/sample - loss: 36.8108 - mean_squared_error: 36.8108 - val_loss: 51.7897 - val_mean_squared_error: 51.7897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x104481518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit(x_train.values, y_train, validation_data=(x_test,y_test), epochs=100, batch_size=20, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification?\n",
    "The only thing we really need to change is the final layer to predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iris = tf.keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "\n",
    "model_iris.add(layers.Dense(100, activation='sigmoid'))\n",
    "model_iris.add(layers.Dense(100, activation='sigmoid'))\n",
    "model_iris.add(layers.Dense(3 ,activation='sigmoid'),)\n",
    "\n",
    "model_iris.compile(optimizer=tf.train.GradientDescentOptimizer(0.01),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= iris['data']\n",
    "y = iris['target']\n",
    "# Why do I not have to scale?\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 21:17:57.737571 4386702784 deprecation.py:323] From /Users/Yejur/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.1406 - acc: 0.3417 - val_loss: 1.1207 - val_acc: 0.3000\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 761us/sample - loss: 1.1186 - acc: 0.3333 - val_loss: 1.1100 - val_acc: 0.3000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 875us/sample - loss: 1.1089 - acc: 0.3167 - val_loss: 1.1138 - val_acc: 0.3000\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 866us/sample - loss: 1.1127 - acc: 0.3000 - val_loss: 1.1069 - val_acc: 0.3000\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 797us/sample - loss: 1.1094 - acc: 0.2500 - val_loss: 1.1039 - val_acc: 0.3000\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 832us/sample - loss: 1.1073 - acc: 0.2583 - val_loss: 1.0977 - val_acc: 0.3000\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1013 - acc: 0.2250 - val_loss: 1.0978 - val_acc: 0.3000\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 925us/sample - loss: 1.1021 - acc: 0.3167 - val_loss: 1.0944 - val_acc: 0.3000\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 808us/sample - loss: 1.0971 - acc: 0.3417 - val_loss: 1.0887 - val_acc: 0.6333\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 786us/sample - loss: 1.0960 - acc: 0.2917 - val_loss: 1.0863 - val_acc: 0.7000\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 1s 4ms/sample - loss: 1.0920 - acc: 0.3917 - val_loss: 1.0834 - val_acc: 0.6333\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0912 - acc: 0.4083 - val_loss: 1.0838 - val_acc: 0.3000\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0882 - acc: 0.3083 - val_loss: 1.0794 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 867us/sample - loss: 1.0875 - acc: 0.4333 - val_loss: 1.0777 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 860us/sample - loss: 1.0858 - acc: 0.3833 - val_loss: 1.0784 - val_acc: 0.3000\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0826 - acc: 0.3333 - val_loss: 1.0736 - val_acc: 0.3000\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 1.0761 - acc: 0.3917 - val_loss: 1.0715 - val_acc: 0.3000\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 1.0758 - acc: 0.4500 - val_loss: 1.0685 - val_acc: 0.3000\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0735 - acc: 0.4917 - val_loss: 1.0669 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 919us/sample - loss: 1.0695 - acc: 0.3750 - val_loss: 1.0594 - val_acc: 0.6667\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 945us/sample - loss: 1.0727 - acc: 0.4833 - val_loss: 1.0571 - val_acc: 0.6667\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0635 - acc: 0.3667 - val_loss: 1.0510 - val_acc: 0.6667\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0625 - acc: 0.5250 - val_loss: 1.0492 - val_acc: 0.6667\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0565 - acc: 0.4500 - val_loss: 1.0417 - val_acc: 0.8667\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 881us/sample - loss: 1.0526 - acc: 0.6000 - val_loss: 1.0405 - val_acc: 0.6667\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0456 - acc: 0.6083 - val_loss: 1.0395 - val_acc: 0.3333\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0421 - acc: 0.4250 - val_loss: 1.0285 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 988us/sample - loss: 1.0325 - acc: 0.6583 - val_loss: 1.0227 - val_acc: 0.6667\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 4ms/sample - loss: 1.0298 - acc: 0.5500 - val_loss: 1.0200 - val_acc: 0.6667\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 929us/sample - loss: 1.0275 - acc: 0.6000 - val_loss: 1.0095 - val_acc: 0.9333\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 777us/sample - loss: 1.0228 - acc: 0.5250 - val_loss: 1.0018 - val_acc: 0.6667\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.0146 - acc: 0.6583 - val_loss: 1.0035 - val_acc: 0.6667\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0053 - acc: 0.6083 - val_loss: 0.9860 - val_acc: 0.9667\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 912us/sample - loss: 1.0007 - acc: 0.6750 - val_loss: 0.9784 - val_acc: 0.9333\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 773us/sample - loss: 0.9900 - acc: 0.6833 - val_loss: 0.9790 - val_acc: 0.6667\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 970us/sample - loss: 0.9830 - acc: 0.6667 - val_loss: 0.9601 - val_acc: 0.9000\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 0.9761 - acc: 0.6583 - val_loss: 0.9604 - val_acc: 0.9333\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 709us/sample - loss: 0.9646 - acc: 0.7333 - val_loss: 0.9557 - val_acc: 0.6667\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.9595 - acc: 0.6583 - val_loss: 0.9313 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.9432 - acc: 0.6667 - val_loss: 0.9333 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 594us/sample - loss: 0.9412 - acc: 0.6917 - val_loss: 0.9124 - val_acc: 0.6667\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 516us/sample - loss: 0.9242 - acc: 0.7083 - val_loss: 0.8976 - val_acc: 0.7000\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 856us/sample - loss: 0.9164 - acc: 0.6750 - val_loss: 0.8910 - val_acc: 0.8000\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 784us/sample - loss: 0.9085 - acc: 0.7167 - val_loss: 0.8749 - val_acc: 0.6667\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 916us/sample - loss: 0.8926 - acc: 0.6917 - val_loss: 0.8640 - val_acc: 0.8333\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 0.8807 - acc: 0.7250 - val_loss: 0.8536 - val_acc: 0.7000\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 795us/sample - loss: 0.8693 - acc: 0.7000 - val_loss: 0.8377 - val_acc: 0.7000\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.8568 - acc: 0.6583 - val_loss: 0.8206 - val_acc: 0.6667\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 792us/sample - loss: 0.8402 - acc: 0.7667 - val_loss: 0.8148 - val_acc: 0.7000\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.8322 - acc: 0.6750 - val_loss: 0.8039 - val_acc: 0.7000\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.8106 - acc: 0.7083 - val_loss: 0.7897 - val_acc: 0.7000\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.8045 - acc: 0.7917 - val_loss: 0.7717 - val_acc: 0.9000\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 979us/sample - loss: 0.7886 - acc: 0.7583 - val_loss: 0.7555 - val_acc: 0.9333\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.7713 - acc: 0.7750 - val_loss: 0.7391 - val_acc: 0.8000\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 983us/sample - loss: 0.7546 - acc: 0.8083 - val_loss: 0.7435 - val_acc: 0.6667\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 869us/sample - loss: 0.7517 - acc: 0.7083 - val_loss: 0.7122 - val_acc: 0.6667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 882us/sample - loss: 0.7314 - acc: 0.7167 - val_loss: 0.6989 - val_acc: 0.7667\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 907us/sample - loss: 0.7224 - acc: 0.7250 - val_loss: 0.6823 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.7088 - acc: 0.7667 - val_loss: 0.6710 - val_acc: 0.9000\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 908us/sample - loss: 0.6931 - acc: 0.7167 - val_loss: 0.6590 - val_acc: 0.7667\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 743us/sample - loss: 0.6808 - acc: 0.7667 - val_loss: 0.6464 - val_acc: 0.8000\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 921us/sample - loss: 0.6714 - acc: 0.7667 - val_loss: 0.6343 - val_acc: 0.8333\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 798us/sample - loss: 0.6578 - acc: 0.7500 - val_loss: 0.6227 - val_acc: 0.9667\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 981us/sample - loss: 0.6486 - acc: 0.8167 - val_loss: 0.6161 - val_acc: 0.7000\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 814us/sample - loss: 0.6348 - acc: 0.7833 - val_loss: 0.6044 - val_acc: 0.7000\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 824us/sample - loss: 0.6226 - acc: 0.7917 - val_loss: 0.5893 - val_acc: 0.9000\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.6177 - acc: 0.8000 - val_loss: 0.5814 - val_acc: 0.8000\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.6065 - acc: 0.7833 - val_loss: 0.5701 - val_acc: 0.9333\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.6032 - acc: 0.8167 - val_loss: 0.5604 - val_acc: 0.9333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5857 - acc: 0.8583 - val_loss: 0.5517 - val_acc: 0.9333\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 877us/sample - loss: 0.5785 - acc: 0.8250 - val_loss: 0.5463 - val_acc: 0.9333\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 860us/sample - loss: 0.5737 - acc: 0.8583 - val_loss: 0.5384 - val_acc: 0.8333\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5646 - acc: 0.8250 - val_loss: 0.5273 - val_acc: 0.9333\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5601 - acc: 0.8500 - val_loss: 0.5199 - val_acc: 0.9333\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 872us/sample - loss: 0.5452 - acc: 0.8333 - val_loss: 0.5132 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 939us/sample - loss: 0.5393 - acc: 0.8833 - val_loss: 0.5072 - val_acc: 0.8667\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 857us/sample - loss: 0.5342 - acc: 0.8417 - val_loss: 0.4992 - val_acc: 0.9333\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5265 - acc: 0.8167 - val_loss: 0.4934 - val_acc: 0.9000\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5221 - acc: 0.8917 - val_loss: 0.4885 - val_acc: 0.8667\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 0.5201 - acc: 0.8917 - val_loss: 0.4840 - val_acc: 0.8333\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 773us/sample - loss: 0.5086 - acc: 0.9333 - val_loss: 0.4845 - val_acc: 0.8000\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.5062 - acc: 0.8083 - val_loss: 0.4748 - val_acc: 0.8333\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4956 - acc: 0.8667 - val_loss: 0.4640 - val_acc: 0.9333\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4973 - acc: 0.9167 - val_loss: 0.4597 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4878 - acc: 0.8917 - val_loss: 0.4540 - val_acc: 0.9333\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 927us/sample - loss: 0.4779 - acc: 0.9250 - val_loss: 0.4588 - val_acc: 0.8000\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 876us/sample - loss: 0.4784 - acc: 0.8917 - val_loss: 0.4569 - val_acc: 0.8000\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 787us/sample - loss: 0.4729 - acc: 0.8667 - val_loss: 0.4405 - val_acc: 0.9333\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 702us/sample - loss: 0.4667 - acc: 0.8667 - val_loss: 0.4368 - val_acc: 0.9667\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 801us/sample - loss: 0.4613 - acc: 0.8833 - val_loss: 0.4308 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4630 - acc: 0.8750 - val_loss: 0.4262 - val_acc: 0.9333\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4551 - acc: 0.9583 - val_loss: 0.4296 - val_acc: 0.8333\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 997us/sample - loss: 0.4468 - acc: 0.8750 - val_loss: 0.4177 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4503 - acc: 0.9250 - val_loss: 0.4135 - val_acc: 0.9667\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4391 - acc: 0.9250 - val_loss: 0.4141 - val_acc: 0.9000\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 0.4377 - acc: 0.9083 - val_loss: 0.4059 - val_acc: 0.9333\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 983us/sample - loss: 0.4343 - acc: 0.9167 - val_loss: 0.4029 - val_acc: 0.9333\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4305 - acc: 0.9333 - val_loss: 0.3983 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.4210 - acc: 0.9000 - val_loss: 0.4018 - val_acc: 0.8667\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 846us/sample - loss: 0.4218 - acc: 0.9250 - val_loss: 0.3916 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2cf5b940>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_iris.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=100, batch_size=4, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 1, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_iris.predict(x_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 1, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
